{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from gensim import corpora, models\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import csv \n",
    "import classifier\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'.')\n",
    "sys.path.insert(0,'/data_big/mlp/custom_lda2vec/lda2vec-pytorch/utils')\n",
    "# from utils import preprocess, get_windows\n",
    "# from utils.preprocess import preprocess\n",
    "from preprocess_mod import *\n",
    "from get_windows_mod import *\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "    \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['via'])\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20newsgroups  README.md\t\t\t      utils\t   y_test.pkl\r\n",
      "LICENSE       semeval2016-task6-domaincorpus  X_test.pkl   y_train.pkl\r\n",
      "loss.png      stance\t\t\t      X_train.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_COUNTS = 10\n",
    "MAX_COUNTS = 1800\n",
    "# words with count < MIN_COUNTS\n",
    "# and count > MAX_COUNTS\n",
    "# will be removed\n",
    "\n",
    "MIN_LENGTH = 3\n",
    "# minimum document length \n",
    "# (number of words)\n",
    "# after preprocessing\n",
    "\n",
    "# half the size of the context around a word\n",
    "HALF_WINDOW_SIZE = 1\n",
    "# it must be that 2*HALF_WINDOW_SIZE < MIN_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('slangs.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile) \n",
    "    slang_words = {rows[0]:rows[1] for rows in reader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(sent):\n",
    "    sent = str(sent)\n",
    "    \n",
    "    # Substitute Slangs\n",
    "    for word in sent.split(\" \"):\n",
    "        if word in slang_words.keys():\n",
    "            sent = re.sub(word, slang_words[word], sent)\n",
    "            \n",
    "    # Remove new line characters\n",
    "    sent = re.sub('\\s+', ' ', sent)\n",
    "\n",
    "    # Remove http:// links\n",
    "    sent = re.sub('http:\\/\\/.*','', sent)\n",
    "\n",
    "    # Remove https:// links\n",
    "    sent = re.sub('https:\\/\\/.*','', sent)\n",
    "    \n",
    "    # Remove distracting single quotes\n",
    "    sent = re.sub(\"\\'\", \"\", sent)\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    sent = re.sub(\"\\\"\", \"\", sent)\n",
    "\n",
    "    # Remove hashtags\n",
    "    sent = re.sub(\"\\#\", \"\", sent)\n",
    "    \n",
    "    sent = sent.lower()\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_LABELLED_DATA_TRUMP = \"../semeval2016-task6-domaincorpus/data-all-annotations/testdata-taskB-all-annotations.txt\"\n",
    "PATH_UNLABELLED_DATA_TRUMP = \"./../semeval2016-task6-domaincorpus/downloaded_Donald_Trump.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle('X_train.pkl')\n",
    "X_test = pd.read_pickle('X_test.pkl')\n",
    "y_train = pd.read_pickle('y_train.pkl')\n",
    "y_test = pd.read_pickle('y_test.pkl')\n",
    "\n",
    "data_labelled_train = pd.concat([X_train, y_train], ignore_index=True, axis=1)\n",
    "data_labelled_train.columns = ['Tweet', 'Stance']\n",
    "data_labelled_test = pd.concat([X_test, y_test], ignore_index=True, axis=1)\n",
    "data_labelled_test.columns = ['Tweet', 'Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_unlabelled = pd.read_csv(PATH_UNLABELLED_DATA_TRUMP, sep='\\t', lineterminator='\\n', encoding ='latin1', names = [\"ID\", \"Tweet\"])\n",
    "data_unlabelled = data_unlabelled.where(data_unlabelled.Tweet != 'Not Available')\n",
    "data_unlabelled.dropna(how='any', inplace=True)\n",
    "data_unlabelled['Tweet'] = data_unlabelled['Tweet'].apply(lambda x: x[1:])\n",
    "data_unlabelled['Tweet'] = data_unlabelled['Tweet'].apply(clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [(i, doc.lower(), \"test\", data_labelled_test['Stance'][i]) for i, doc in enumerate(data_labelled_test['Tweet'])]\n",
    "docs += [(i, doc.lower(), \"train\", data_labelled_train['Stance'][i]) for i, doc in enumerate(data_labelled_train['Tweet'])]\n",
    "docs += [(i, doc.lower(), \"unlabelled\", \"UNK\") for i, doc in enumerate(data_unlabelled['Tweet'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54513"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  'we need a man that will do the tough negotiations, say what needs to be '\n",
      "  'said and forget the words politically correct. thanks semst',\n",
      "  'test',\n",
      "  'AGAINST'),\n",
      " (1,\n",
      "  'we love mexicans,we respect your work ethic,your love of family,your '\n",
      "  'loyalty,your food &your love of god! lets all get jobs,vote semst',\n",
      "  'test',\n",
      "  'AGAINST'),\n",
      " (2,\n",
      "  'extremistprogressives are so focused on their agenda that they believe a '\n",
      "  'inanimate flagkills but multitimedeporteesdont semst',\n",
      "  'test',\n",
      "  'NONE'),\n",
      " (3,\n",
      "  '@braveconwarrior stories like this make think patriots are at the end of '\n",
      "  'the string. all kinds of shit is going to hit fan semst',\n",
      "  'test',\n",
      "  'AGAINST'),\n",
      " (4,\n",
      "  'gop candidate predictions? america decides to play the trump card and go '\n",
      "  'with the man with the plan. semst',\n",
      "  'test',\n",
      "  'FAVOR')]\n"
     ]
    }
   ],
   "source": [
    "pprint(docs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess dataset and create windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 11347/54513 [01:45<06:46, 106.08it/s]"
     ]
    }
   ],
   "source": [
    "encoded_docs, decoder, word_counts = preprocess(\n",
    "    docs, nlp, MIN_LENGTH, MIN_COUNTS, MAX_COUNTS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new ids will be created for the documents.\n",
    "# create a way of restoring initial ids:\n",
    "doc_decoder = {i: doc_id for i, (doc_id, doc, type, stance) in enumerate(encoded_docs)}\n",
    "doc_decoder_reverse = {doc_id : i for i, (doc_id, doc, type, stance) in enumerate(encoded_docs)} \n",
    "# doc_decoder_all = {i: docs[doc_id] for i, (doc_id, doc, type, stance) in enumerate(encoded_docs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "test_data = []\n",
    "all_data = []\n",
    "# new ids are created here\n",
    "for index, (_, doc, type, stance) in tqdm(enumerate(encoded_docs)):\n",
    "    windows = get_windows(doc, HALF_WINDOW_SIZE)\n",
    "    # index represents id of a document, \n",
    "    # windows is a list of (word, window around this word),\n",
    "    # where word is in the document\n",
    "    if type == \"train\" or type == \"unlabelled\":\n",
    "        data += [[index, w[0]] + w[1] for w in windows]\n",
    "    if type == \"test\":\n",
    "        test_data += [[index, w[0]] + w[1] for w in windows]\n",
    "    all_data += [[index, w[0]] + w[1] for w in windows]\n",
    "\n",
    "data = np.array(data, dtype='int64')\n",
    "test_data = np.array(test_data, dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a row in 'data' contains:\n",
    "# id of a document, id of a word in this document, a window around this word\n",
    "# 1 + 1 + 10\n",
    "data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of windows (equals to the total number of tokens)\n",
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get unigram distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = np.array(word_counts)\n",
    "unigram_distribution = word_counts/sum(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "vocab_size = len(decoder)\n",
    "embedding_dim = 100\n",
    "\n",
    "# train a skip-gram word2vec model\n",
    "texts = [[str(j) for j in doc] for i, doc, type, stance in encoded_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Word2Vec(texts, size=embedding_dim, window=5, workers=4, sg=1, negative=15, iter=70)\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = np.zeros((vocab_size, embedding_dim)).astype('float32')\n",
    "for i in decoder:\n",
    "#     print(str(i))\n",
    "    if str(i) == '3469':\n",
    "        continue\n",
    "    word_vectors[i] = model.wv[str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique words\n",
    "print(len(word_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Prepare initialization for document weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[decoder[j] for j in doc] for i, doc, type, stance in encoded_docs]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "# !unzip mallet-2.0.8.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 20\n",
    "mallet_path = './mallet-2.0.8/bin/mallet' # update this path\n",
    "ldamallet = models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=n_topics, id2word=dictionary)\n",
    "ldamallet = models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)\n",
    "corpus_lda_ldamallet = ldamallet[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# lda = models.LdaModel(corpus, alpha=0.9, id2word=dictionary, num_topics=n_topics)\n",
    "# corpus_lda = lda[corpus]\n",
    "lda = ldamallet\n",
    "corpus_lda = corpus_lda_ldamallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, topics in lda.show_topics(n_topics, formatted=False):\n",
    "    print('topic', i, ':', ' '.join([t for t, _ in topics]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_weights_init = np.zeros((len(corpus_lda), n_topics))\n",
    "for i in tqdm(range(len(corpus_lda))):\n",
    "    topics = corpus_lda[i]\n",
    "    for j, prob in topics:\n",
    "        doc_weights_init[i, j] = prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mallet_feats = []\n",
    "\n",
    "# for i in range(len(corpus_lda)):\n",
    "#     representation = lda.get_document_topics(corpus_lda[i])\n",
    "#     count = 0\n",
    "#     feat = []\n",
    "#     for i in range(n_topics):\n",
    "#         if i in list(map((lambda x: x[0]), representation)):\n",
    "#             feat.append(representation[count][1])\n",
    "#             count = count + 1\n",
    "#         else:\n",
    "#             feat.append(0)\n",
    "#     mallet_feats.append(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mallet_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_weights_init = np.array(mallet_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_weights_init.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels(label):\n",
    "    if label == 'AGAINST':\n",
    "        return 0\n",
    "    elif label == 'FAVOR':\n",
    "        return 1\n",
    "    elif label == 'NONE':\n",
    "        return 2\n",
    "\n",
    "x_train = []\n",
    "x_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "# doc_decoder is mapping from new id to old id\n",
    "# for i in range(len(doc_decoder)):\n",
    "# print(doc_decoder)\n",
    "encoded_docs_dict = {}\n",
    "for i, (j, doc, type, stance) in enumerate(encoded_docs):\n",
    "#     print(doc, stance)\n",
    "#     if int(j) in doc_decoder_reverse:\n",
    "#         i = doc_decoder_reverse[int(j)]\n",
    "    encoded_docs_dict[i] = [j, type, stance]\n",
    "    if type == \"train\":\n",
    "        y_train.append(transform_labels(stance))\n",
    "        x_train.append(doc_weights_init[i].tolist())\n",
    "    if type == \"test\":\n",
    "        y_test.append(transform_labels(stance))\n",
    "        x_test.append(doc_weights_init[i].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_train), len(x_train[0]))\n",
    "print(len(y_train))\n",
    "print(len(x_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred, mallet_report = classifier.RandomForest(x_train, x_test, y_train, y_test)\n",
    "# pprint(mallet_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('data.npy', data)\n",
    "np.save('docs.npy', docs)\n",
    "np.save('encoded_docs.npy', encoded_docs_dict)\n",
    "np.save('data.npy', all_data)\n",
    "np.save('data_ids.npy', data[:,0])\n",
    "np.save('test_data.npy', test_data)\n",
    "np.save('word_vectors.npy', word_vectors)\n",
    "np.save('unigram_distribution.npy', unigram_distribution)\n",
    "np.save('decoder.npy', decoder)\n",
    "np.save('doc_decoder.npy', doc_decoder)\n",
    "np.save('doc_decoder_reverse.npy', doc_decoder_reverse)\n",
    "# np.save('doc_decoder_all.npy', doc_decoder_all)\n",
    "np.save('doc_weights_init.npy', doc_weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[269])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_torch_custom)",
   "language": "python",
   "name": "conda_torch_custom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
